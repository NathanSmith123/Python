{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6 Lasso Assignment\n",
    "## By Nathan Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data set\n",
    "alldata = pd.read_csv('https://www.dropbox.com/s/0teqe6jwpryc28j/finalmaster-ratios.csv?dl=1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of the column headers for analysis\n",
    "allvariablenames = list(alldata.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the first 7 value of the collumn headers from the list\n",
    "listofallpredictors = allvariablenames[8:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load predictors into dataframe\n",
    "predictors = alldata[listofallpredictors]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load target into dataframe\n",
    "target = alldata['# Purchases'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=1.134e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.011e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=5.919e-02, with an active set of 77 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 98 iterations, alpha=5.774e-02, previous alpha=5.717e-02, with an active set of 79 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.095e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 18 iterations, alpha=7.200e-01, previous alpha=7.200e-01, with an active set of 11 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.916e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.645e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.456e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.867e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.734e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.709e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.692e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=1.668e-01, previous alpha=1.607e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 5.867e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.368e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.031e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.535e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=7.535e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.298e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.144e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=7.357e-01, previous alpha=6.074e-01, with an active set of 10 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 4.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.554e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.277e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.089e-01, with an active set of 28 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.880e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=1.742e-01, previous alpha=1.733e-01, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\N\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# create the model using LASSO\n",
    "        \n",
    "model = LassoLarsCV(cv = 10, precompute = False).fit(pred_train, tar_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01001036' 2.7861365955132498]\n",
      "['B01001037' 0.9200572652790083]\n",
      "['B01001038' 0.9459340522644347]\n",
      "['B02001005' 0.3915680921615554]\n",
      "['B13014026' 0.22056164158451877]\n",
      "['B13014027' 0.05049787197081122]\n",
      "['B19001017' 1.6062678580473926]\n"
     ]
    }
   ],
   "source": [
    "# build the coefficient chart\n",
    "\n",
    "predictors_model = pd.DataFrame(listofallpredictors)\n",
    "predictors_model.columns = ['label']\n",
    "predictors_model['coeff'] = model.coef_\n",
    "\n",
    "for index, row in predictors_model.iterrows():\n",
    "    if row['coeff'] > 0:\n",
    "        print(row.values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "============================== Question #1 ==============================\n",
      " \n",
      " \n",
      "predictors_model=pd.DataFrame(listofallpredictors)\n",
      "predictors_model.columns = ['label']\n",
      "predictors_model['coeff'] = model.coef_\n",
      " \n",
      "for index, row in predictors_model.iterrows():\n",
      "    if row['coeff'] > 0:\n",
      "        print(row.values)\n",
      " \n",
      " \n",
      "In your own words, explain what the above lines of code\n",
      "are doing. Why am I doing it? Explain each line.\n",
      " \n",
      " \n",
      " \n",
      "Answer: The first line creates the predictor_models dataframe out of the 182 predictors.\n",
      "The second line names the column that contains the predictors.\n",
      "The third line creates a new column named coeff and stores the coefficient values for that predictor.\n",
      "The for loop iterates through the predictors_model dataframe and prints the predictor and the coefficient if the coefficient had a value greater than 0.\n",
      "========================================================================\n",
      "============================== Question #2 ==============================\n",
      " \n",
      " \n",
      "Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? \n"
     ]
    }
   ],
   "source": [
    "# Question #1\n",
    "    \n",
    "print(' ')   \n",
    "print(' ')   \n",
    "print(' ')   \n",
    "print('============================== Question #1 ==============================')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('predictors_model=pd.DataFrame(listofallpredictors)')\n",
    "print('predictors_model.columns = [\\'label\\']')\n",
    "print('predictors_model[\\'coeff\\'] = model.coef_')\n",
    "print(' ')\n",
    "print('for index, row in predictors_model.iterrows():')\n",
    "print('    if row[\\'coeff\\'] > 0:')\n",
    "print('        print(row.values)')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('In your own words, explain what the above lines of code') \n",
    "print('are doing. Why am I doing it? Explain each line.')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Answer: The first line creates the predictor_models dataframe out of the 182 predictors.')\n",
    "print('The second line names the column that contains the predictors.')\n",
    "print('The third line creates a new column named coeff and stores the coefficient values for that predictor.')\n",
    "print('The for loop iterates through the predictors_model dataframe and prints the predictor and the coefficient if the coefficient had a value greater than 0.')\n",
    "\n",
    "print('========================================================================')\n",
    "\n",
    "print('============================== Question #2 ==============================')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "The consumer userbase of this product is broken down into the following demographic segments:\n",
      "B01001036: Sex by age, Female between 30 - 34 years\n",
      "B01001037: Sex by age, Female between 35 - 39 years\n",
      "B01001038: Sex by age, Female between 40 - 44 years\n",
      "B02001005: Race, Asian alone\n",
      "B13014026: Women who have not had a baby in the past 12 months, unmaried, with a bachelors degree\n",
      "B13014027: Women who have not had a baby in the past 12 months, unmaried, with a gradute or professional degree\n",
      "B19001017: Houseshold income from past 12 months, $200,000+\n",
      "Practically this means that areas that contain the listed demographics result in higher quantity of product sold\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Question #2 section. look up the variable names\n",
    "\n",
    "# B01001036: Sex by age, Female between 30 - 34 years\n",
    "# B01001037: Sex by age, Female between 35 - 39 years\n",
    "# B01001038: Sex by age, Female between 40 - 44 years\n",
    "# B02001005: Race, Asian alone\n",
    "# B13014026: Women who have not had a baby in the past 12 months, unmaried, with a bachelors degree\n",
    "# B13014027: Women who have not had a baby in the past 12 months, unmaried, with a gradute or professional degree\n",
    "# B19001017: Houseshold income from past 12 months, $200,000 +\n",
    "\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')\n",
    "\n",
    "print('The consumer userbase of this product is broken down into the following demographic segments:')\n",
    "print('B01001036: Sex by age, Female between 30 - 34 years')\n",
    "print('B01001037: Sex by age, Female between 35 - 39 years')\n",
    "print('B01001038: Sex by age, Female between 40 - 44 years')\n",
    "print('B02001005: Race, Asian alone')\n",
    "print('B13014026: Women who have not had a baby in the past 12 months, unmaried, with a bachelors degree')\n",
    "print('B13014027: Women who have not had a baby in the past 12 months, unmaried, with a gradute or professional degree')\n",
    "print('B19001017: Houseshold income from past 12 months, $200,000+')\n",
    "print('Practically this means that areas that contain the listed demographics result in higher quantity of product sold')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Question #3 ==============================\n",
      "If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?\n",
      "The top consumer demographic segments for this product are:\n",
      " # 1: B01001036: Sex by age, Female between 30 - 34 years : Coeff = 2.7861365955132498\n",
      " # 2: B19001017: Houseshold income from past 12 months, $200,000+ : Coeff = 1.6062678580473926\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('============================== Question #3 ==============================')\n",
    "print('If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?')\n",
    "print('The top consumer demographic segments for this product are:')\n",
    "print(' # 1: B01001036: Sex by age, Female between 30 - 34 years : Coeff = 2.7861365955132498')\n",
    "print(' # 2: B19001017: Houseshold income from past 12 months, $200,000+ : Coeff = 1.6062678580473926')\n",
    "print(' ')\n",
    "print(' ')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Question #4 ==============================\n",
      "Are the training and text set mean squared errors similar? What does that mean practically?\n",
      " \n",
      "training data MSE\n",
      "22528.486826258624\n",
      " \n",
      " \n",
      "test data MSE\n",
      "41578.280293705764\n",
      " \n",
      " \n",
      "The MSE for both data sets are different.\n",
      "The training data MSE is lower which means that our model is better at predicting within the training data set than it is at predicting within the test data set.\n",
      "This makes sense as the training data contains many more observations for the model to observe and practice with.\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('============================== Question #4 ==============================')\n",
    "print('Are the training and text set mean squared errors similar? What does that mean practically?')\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(' ')  \n",
    "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "print ('training data MSE')\n",
    "print(train_error) \n",
    "print(' ')\n",
    "print(' ')\n",
    "train_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('test data MSE')\n",
    "print(train_error)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('The MSE for both data sets are different.') \n",
    "print('The training data MSE is lower which means that our model is better at predicting within the training data set than it is at predicting within the test data set.')\n",
    "print('This makes sense as the training data contains many more observations for the model to observe and practice with.')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Question #5 ==============================\n",
      "If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?\n",
      "training data R-square\n",
      "0.22266652028942102\n",
      " \n",
      "test data R-square\n",
      "0.17529294561525344\n",
      " \n",
      " I would advise my boss that this model is not very good at predicting sales based on the census data. The R-squared values are very low. The model needs furthur testing and fitting in order to gain accuracy.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('============================== Question #5 ==============================')\n",
    "print('If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?')\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "print ('training data R-square')\n",
    "print(rsquared_train)\n",
    "print(' ')\n",
    "rsquared_test=model.score(pred_test,tar_test)\n",
    "print ('test data R-square')\n",
    "print(rsquared_test)\n",
    "print(' ')\n",
    "print(' I would advise my boss that this model is not very good at predicting sales based on the census data. The R-squared values are very low. The model needs furthur testing and fitting in order to gain accuracy.')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Question #6 ==============================\n",
      " \n",
      "What is our baseline sales number? What does that mean, practically?\n",
      "y interecept:\n",
      "2.7587387103223477\n",
      " \n",
      "Our baseline sales number is 2.75, so somewhere between 2 and 3 bars sold.\n",
      "This means that the alpha for our regression is 2.75, which is how many bars we sold when all betas were at 0.\n",
      "If none of the demographic market segments listed above existed we would sell 2.75 bars.\n"
     ]
    }
   ],
   "source": [
    "print('============================== Question #6 ==============================')\n",
    "print(' ')\n",
    "print('What is our baseline sales number? What does that mean, practically?')     \n",
    "print(\"y interecept:\")\n",
    "print(model.intercept_)\n",
    "print(' ')\n",
    "print('Our baseline sales number is 2.75, so somewhere between 2 and 3 bars sold.') \n",
    "print('This means that the alpha for our regression is 2.75, which is how many bars we sold when all betas were at 0.') \n",
    "print('If none of the demographic market segments listed above existed we would sell 2.75 bars.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
